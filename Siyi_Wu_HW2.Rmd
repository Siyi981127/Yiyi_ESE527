---
title: 'HW2: Outlier Detection Review'
author: "Siyi Wu"
date: "2022/2/26"
output:
  html_document:
    keep_md: yes
    number_sections: yes
  word_document: default
  pdf_document: default
---
This homeword is devided to two parts, problem1(using iris data) and problem2(using our data).
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Deep-Dive on Outlier detection Methods


```{r}
library(OutlierDetection)
library(OutliersO3)
library(outliers)
```


## Data Description:

```{r iris}
help(iris)
summary(iris)
head(iris)

```


## Problem 1: Expanding knowledge based on Outlier detection techniques


### 1.-Statistical Tests based Approaches:

#### a) Dixon test (small sample size)

Technical Summary:
A way to find outliers in very small, normally distributed, data sets. 
Critical values at the 95% confidence level for the two-tailed Q test, and related tests based upon subrange ratlos, for the statistical rejectlon of outlying data have been Interpolated by applying cubic regresslon analysis to the values originally published by Dixon. Corrections to errors In Dixon's original tables are also Included. The resultant values are judged to be accurate to within +0.002 and corroborate the fact that corresponding critical values published in recent statlstical treatises for analytical chemists are erroneous. It is recommended that the newly generated 95% critical values be adopted by analytical chemists as the general standard for the rejection of outller values.

References:
- Dixon, W.J. (1950). Analysis of extreme values. Ann. Math. Stat. 21, 4, 488-506.
- Dixon, W.J. (1951). Ratios involving extreme values. Ann. Math. Stat. 22, 1, 68-78.
- Rorabacher, D.B. (1991). Statistical Treatment for Rejection of Deviant Values: Critical Values of Dixon Q Parameter and Related Subrange Ratios at the 95 percent Confidence Level. Anal. Chem.
83, 2, 139-146.

Application:

```{r}
X=iris[1:30,1]
dixon.test(X,type=0,opposite=TRUE)

```
#### b) Normalscore (Deviation with respect to the mean)

Technical Summary:
This function calculates normal scores of given data.

References:
Schiffler, R.E (1998). Maximum Z scores and outliers. Am. Stat. 42, 1, 79-80.

Application:

```{r}
X=iris[,1:4]
#scores(X,type="z",prob=0.95)
#Displaying first 10 scores
scores(X,type="z",prob=0.95)[1:10,]

```


#### c) Median Absolute Deviation (Deviation with respect to the median)

Technical Summary:
This function gives differences between each value and median, divided by median absolute deviation.

References:
Schiffler, R.E (1998). Maximum Z scores and outliers. Am. Stat. 42, 1, 79-80.

```{r}
X=iris[,1:4]
#scores(X,type="mad",prob=0.95)
#Displaying first 10 scores
scores(X,type="mad",prob=0.95)[1:10,]

```


#### d) Interquantile range score

Technical Summary:
All values lower than first and greater than third quartile is considered, and difference between them and nearest quartile divided by IQR are calculated.

References:
Schiffler, R.E (1998). Maximum Z scores and outliers. Am. Stat. 42, 1, 79-80.

Note: check for the value of limit to be used. Below I inserted an arbitrary value
```{r}
X=iris[,1:4]
#scores(X,type="iqr",lim=1)
#Displaying first 10 scores
scores(X,type="iqr",lim=1)[1:10,]
```


### 2. Depth-based Approach:

Technical Summary:
Get the data set and look for its outliers using depth-based methods. Outlier detection method based on depth

Reference:
Johnson, T., Kwok, I., and Ng, R.T. 1998. Fast computation of 2-dimensional depth contours. In Proc. Int. Conf. on Knowledge Discovery and Data Mining (KDD), New York, NY. Kno

Application:

```{r}
X=iris[,1:4]
depthout(X,cutoff=0.05)



```

### 3. Deviation-based Approaches
Technical Summary:
It identifies outliers by examining the main characteristics of objects in a group. Objects that "deviate" from this description are considered outliers. Hence, in this approach the term deviations are typically used to refer to outliers.

References:
A. Arning, R. Agrawal, and P. Raghavan. A linear method for deviation detection in large
databases. In Proc. 2nd International Conference on Knowledge Discovery and Data Mining,
1996
Chaudhary, A., Szalay, A. S., and Moore, A. W. 2002. Very fast outlier detection in large multidimensional data sets. In Proceedings of the ACM SIGMOD Workshop in Research Issues in Data Mining and Knowledge Discovery (DMKD). ACM Press


### 4. Distance-based Approaches
#### a) Outlier detection using Mahalanobis Distance
Technical Summary:
Get the data set and find its outliers using a model-based approach. Outlier detection based on Mahalanobis distance
maha(x, cutoff = 0.95, rnames = FALSE)
x: Data set to find outliers
cutoff: Percentage threshold for distance. Default is 0.95
rnames: Indicates whether the dataset has a logical value for row names. The default value is False

References:
Barnett, V. 1978. The study of outliers: purpose and model. Applied Statistics, 27(3), 242â€“250.

Application:
```{r}
X=iris[,1:4]
maha(X,cutoff=0.9)
```

#### b) Outlier detection using k Nearest Neighbours Distance method
Technical Summary:
Gets the data set and looks for its outliers using distance-based methods.Outlier detection based on k nearest Neighbor distance method
nn(x, k = 0.05 * nrow(x), cutoff = 0.95, Method = "euclidean",rnames = FALSE, boottimes = 100)
x: Data set to find outliers
k: The number of nearest neighbors to use, default 0.05*nrow (x)
cutoff: Percentage threshold for distance. Default is 0.95
method: Distance Method. The default value is Euclid
rnames: Indicates whether the dataset has a logical value for row names. The default value is False
boottimes: The number of Bootsrap samples found at the cutoff point. The default is 100 samples

References:
Hautamaki, V., Karkkainen, I., and Franti, P. 2004. Outlier detection using k-nearest neighbour graph. In Proc. IEEE Int. Conf. on Pattern Recognition (ICPR), Cambridge, UK.

Application:

```{r}
X=iris[,1:4]
nn(X,k=4)
```

#### c) Outlier detection using kth Nearest Neighbour Distance method
Technical Summary:
Outlier detection based on k-nearest Neighbor distance method. K is the number of points around the outlier that need to be judged to calculate the local outlier factor

References:
Hautamaki, V., Karkkainen, I., and Franti, P. 2004. Outlier detection using k-nearest neighbour graph. In Proc. IEEE Int. Conf. on Pattern Recognition (ICPR), Cambridge, UK.

Application:

```{r}
X=iris[,1:4]
nnk(X,k=4)
```


### 5. Density-based Approaches
#### a) Outlier detection using Robust Kernal-based Outlier Factor(RKOF) algorithm
Technical Summary:
Outlier detection based on Robust kernel outlier factor (RKOF) algorithm
Reference:
Ester, M., Kriegel, H.-P., Sander, J., and Xu, X. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. Int. Conf. on Knowledge Discovery and Data Mining (KDD), Portland, OR.

Application:
```{r}
X=iris[,1:4]
dens(X,k=4,C=1)
```
#### b) Outlier detection using genralised dispersion
Technical Summary:
Outlier detection based on generalized discrete.

Reference:
Jin, W., Tung, A., and Han, J. 2001. Mining top-n local outliers in large databases. In Proc. ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (SIGKDD), San Francisco, CA.

Application:
```{r}
X=iris[,1:4]
disp(X,cutoff=0.99)
```

### 6. Join assessment of outlier detection methods using techniques described under 2 to 5.

Technical Summary: Outlier detection (intersection of all methods). Given the abudance of method to define outliers a most recent strategy is to develop consensus outlier detection method. For example, rules such as majority vote can be applied when the techniques considered are essentially different. Per instance, see "Outlier detection" package function OutlierDetection which finds outlier observations for the data using different methods and labels an observation as outlier based on the intersection of all the methods considered. Using the function edit in R investigate the criterion being used and which techniques were considered. Also, proposed a modification to the function so to consider any technique to include any given number of techniques for outlier detection. Per instance, ensure that you can include the techniques covered under category 1.

Application:
```{r}
X=iris[,1:4]
OutlierDetection(X)
#Unveil the criterion used in OutlierDection function to define outliers using different methods
#edit(OutlierDetection) # uncomment and execute this line
```


## Problem 2: 

Problem2
#Deep-Dive on Outlier detection Methods



```{r}
library(OutlierDetection)
library(OutliersO3)
library(outliers)
library(stats)
```


## Data Description:


```{r load data}
covid <- readr::read_csv("C:/Users/12228/Desktop/CourseMaterial/SP2022/ESE 527 Practicum in Data Analytics & Statistics/hw2/train.csv")
covid

```


```{r filter}
#china = filter(covid,Country_Region =="China" |Country_Region=="Taiwan*")
china <- covid[which(covid$Country_Region %in% "China"),]
```


## Problem 2: Expanding knowledge based on Outlier detection techniques


### 1.-Statistical Tests based Approaches:

#### a) Dixon test (small sample size)

```{r}
X=covid[1000:1029,5]
X=as.numeric(unlist(X))
dixon.test(X,type=0,opposite=TRUE)

```
#### b) Normalscore (Deviation with respect to the mean)

```{r}
X=china[,5:6]
#scores(X,type="z",prob=0.95)
#Displaying first 10 scores
scores(X,type="z",prob=0.95)[1:10,]
```


#### c) Median Absolute Deviation (Deviation with respect to the median)

```{r}
X3=china[,5:6]
#scores(X,type="mad",prob=0.95)
#Displaying first 10 scores
scores(X3,type="mad",prob=0.95)[1:10,]
```


#### d) Interquantile range score

```{r}
X4=china[,5:6]
#scores(X,type="iqr",lim=1)
#Displaying first 10 scores
scores(X4,type="iqr",lim=1)[1:10,]
```


### 2. Depth-based Approach:


```{r}
X5=china[,5:6]
X5 = as.data.frame(X5)

depthout(X5,cutoff=0.3)

```

### 3. Deviation-based Approaches

### 4. Distance-based Approaches
#### a) Outlier detection using Mahalanobis Distance
```{r}
X11=china[,5:6]
X11 = as.data.frame(X11)
maha(X11,cutoff=0.7)
```

#### b) Outlier detection using k Nearest Neighbours Distance method

```{r}
X22=china[,5:6]
X22 = as.data.frame(X22)
nn(X22,k=4)
```

#### c) Outlier detection using kth Nearest Neighbour Distance method

```{r}
X23=china[,5:6]
X23 = as.data.frame(X23)
nnk(X23,k=10)


```


### 5. Density-based Approaches
#### a) Outlier detection using Robust Kernal-based Outlier Factor(RKOF) algorithm
```{r}
X33=china[,5:6]
X33 = as.data.frame(X33)
X33[is.na(X33)]<-0
#dens(X33,k=3,C=01)
```
#### b) Outlier detection using genralised dispersion

```{r}
xx=china[,5:6]
xx = as.data.frame(xx)
disp(xx,cutoff=0.99)
```

### 6. Join assessment of outlier detection methods using techniques described under 2 to 5.

```{r}
xx1=china[,5:6]
xx1 = as.data.frame(xx1)
OutlierDetection(xx1)
#Unveil the criterion used in OutlierDection function to define outliers using different methods
#edit(OutlierDetection) # uncomment and execute this line
```


```{r}
library(DMwR2)

```

#compute the outlier scores you need to plot the density graph
```{r}
outlier.scores <- lofactor(xx, k=5)
a = outlier.scores
a[is.na(a)]<-0
a
plot(density(a))
```
#obtain the top 5 outliers in the data set
```{r}
outliers <- order(outlier.scores, decreasing=T)[1:5]

print(outliers)
```
#visualize the outliers with a biplot of the first two principal components
```{r}
n <- nrow(xx)

labels <- 1:n

labels[-outliers] <- "."

biplot(prcomp(xx), cex=.8, xlabs=labels)
```
```{r}
pch <- rep(".", n)

pch[outliers] <- "+"

col <- rep("black", n)


col[outliers] <- "red"

pairs(xx, pch=pch, col=col)
```



```{r}
knitr::knit_hooks$set(evaluate = evaluate::evaluate)
```

















